# SoC_2025_EchoChamber
I started off with exploring the Python programming language through the  GeeksforGeeks website which helped me understand the basic syntax, data types, loops, conditionals, and functions. This provided a solid foundation to work with libraries like NumPy and Pandas.

For NumPy, I watched a YouTube video from freeCodeCamp.org which provided background information on how NumPy works and how it compares to Python’s built-in lists. The video started off with the basics of creating arrays, accessing/ changing specific elements, rows and columns. The video also provided an insightful view into how basic mathematics, linear algebra and statistics can be used for the arrays. Then the video proceeded with more advanced stuff like reorganisation, advanced indexing and boolean masking. 

Using another GeeksforGeeks website, I learnt about Pandas and how to work with Series and DataFrames. I found the section on working with CSV files particularly useful, as it showed how to read and manipulate real-world datasets in an efficient and structured way.

For Matplotlib, I watched a YouTube video by NeuralNine and was really fascinated by the different things that could be done using Matplotlib. The video gave an insight into the different kinds of plots that can be plotted for your data like- Scatter, Line, Bar, Histograms, Pie charts etc. I also learnt about different ways to style your plot, how to get multiple figures, subplots. I found the last part of the video really fascinating as it was all about 3D Plotting and animating the plots.   

Going ahead with the week 2 resources, this is when it started getting fun for me. I started watching the YouTube playlist Machine Learning Specialisation by Andrew NG where he first introduced supervised and unsupervised learning, regression and classification. I learnt about cost, function, gradient descent, learning rate, feature scaling and engineering of regression. For classification, I learnt about the logistic regression model, a different version of the cost function since the one used previously wouldnt give a satisfactory result. I also learnt about the loss function, overfitting and regularisation. 

Then I moved on to an artificial neural networks YouTube playlist, which used most of the concepts that I mentioned above and implemented it into a model with input, hidden and output layers. I learnt about forward and backward propagation, a bunch of different functions like tanh and RELU that can be used for the hidden layers, softmax and sigmoid that can be used for the output layers. There was also a video on the derivation of back propagation which was quite insightful.  At the end of the playlist, I implemented a Handwritten Digit Recognition model alongside a video tutorial from the same playlist which was really cool and fascinating. 
